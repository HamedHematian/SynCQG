# -*- coding: utf-8 -*-

# importing packages
import numpy as np
import torch
from tqdm import tqdm
from copy import deepcopy
from transformers.optimization import get_linear_schedule_with_warmup
from transformers import BertModel, AutoTokenizer
import os, gdown, wget
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
import random
from numpy.random import MT19937
from numpy.random import RandomState, SeedSequence
import torch.backends.cudnn
import torch.cuda
import spacy
import json
import zipfile


from eval import *
from argparser import cotah_parse_args
from utils import *
from post_processing import *
from data_structures import CQA_DATA, CoTaH_Feature
from dataloader import DataManager, Cotah_DataLoader
from savings import *
from settings import *

# intiate the spacy
nlp = spacy.load("en_core_web_sm")

# parse arguments
args = cotah_parse_args()


# Setting user-defined hyperparameters
tau = 0 # initial amount for tau
S = args.S
kl_ratio = args.kl_ratio
use_sim_threshold = bool(int(args.use_sim_threshold))
dist_type = args.dist_type # Whether uniform or linear (you can add more distributions by changing the code)
batch_size = args.batch_size
tau = args.tau
SEED = args.seed
gamma_threshold = args.gamma # gamma
M = args.M # M in the paper
accumulation_steps = args.accumulation_steps
source_directory = args.source_directory # saving directory


# Setting necessary settings for reproducibility
def set_determenistic_mode(SEED, disable_cudnn=False):
  torch.manual_seed(SEED)                       # Seed the RNG for all devices (both CPU and CUDA).
  random.seed(SEED)                             # Set python seed for custom operators.
  rs = RandomState(MT19937(SeedSequence(SEED))) # If any of the libraries or code rely on NumPy seed the global NumPy RNG.
  np.random.seed(SEED)
  torch.cuda.manual_seed_all(SEED)              # If you are using multi-GPU. In case of one GPU, you can use # torch.cuda.manual_seed(SEED).

  if not disable_cudnn:
    torch.backends.cudnn.benchmark = False    # Causes cuDNN to deterministically select an algorithm,
                                              # possibly at the cost of reduced performance
                                              # (the algorithm itself may be nondeterministic).
    torch.backends.cudnn.deterministic = True # Causes cuDNN to use a deterministic convolution algorithm,
                                              # but may slow down performance.
                                              # It will not guarantee that your training process is deterministic
                                              # if you are using other libraries that may use nondeterministic algorithms
  else:
    torch.backends.cudnn.enabled = False # Controls whether cuDNN is enabled or not.
                                         # If you want to enable cuDNN, set it to True.
set_determenistic_mode(SEED)
def seed_worker(worker_id):
    worker_seed = SEED
    np.random.seed(worker_seed)
    random.seed(worker_seed)
g = torch.Generator()
g.manual_seed(SEED)


# make the directories to save the data in
make_dir('examples')
make_dir('features')
make_dir('examples/train')
make_dir('examples/eval')
make_dir('examples/test')
make_dir('features/train')
make_dir('features/eval')
make_dir('features/test')
make_dir('features/truth')  


# download quac files
wget.download(quac_train_url)
wget.download(quac_eval_url)
# read quac files
train_data = read_json(train_path)
eval_data = read_json(eval_path)



# printing settings
print('------------- Running Settings -------------')
print('--------------------------------------------')
print('S', S)
print('kl_ratio', kl_ratio)
print('use_sim_threshold', use_sim_threshold)
print('dist_type', dist_type)
print('seed', SEED)
print('batch size', batch_size)
print('accumulation steps', accumulation_steps)
print('tau', tau)
print('gamma', gamma_threshold)
print('M', M)
print('source directory', args.source_directory)
print('--------------------------------------------')
print('--------------------------------------------')


# loading bert
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = BertModel.from_pretrained(model_id)
  
# loading generated synthetic questions
if not os.path.exists('extra_hist.pk'):
  print('Downloading Generated Synthetic Questions ...')
  gdown.download(id=synthetic_questions_id)
  
  
  
# [PH] token is delf-defined special token and added to the tokenizer
# This token is a dummy token to be added to the the end of questions 
# so that the the length of questions be similar for the two QA network runs
special_tokens_dict = {'additional_special_tokens': ['[PH]']}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
model.resize_token_embeddings(len(tokenizer))
hist_chunk_dict = dict()




    
# loading generated synthetic questions. The generated synthtic questions are primarily generated by synthetic answers from noun phrases, adjectives, and adverbs
synthetic_questions = load_data('extra_hist.pk')
id_2_num = dict()
for dialog in train_data['data']:
  dialog = dialog['paragraphs'][0]
  id_2_num[dialog['qas'][0]['id'].split('#')[0]] = len(dialog['qas'])
extra_hists_dict = dict()
for k, v in synthetic_questions.items():
  extra_hists_dict[k] = v




# Examples
def make_new_history_with_heuristic(q_list_, q_num, syn_num):
  real_questions = []
  real_questions_repr = []
  augmented_history = []
  q_list = []

  temp = -1
  for q in q_list_:
    if not q['is_syn']:
      temp += 1
    if temp == q_num:
      main_q_repr = q['repr']
      break
    q_list.append(q)

  for turn in q_list:
    if not turn['is_syn']:
      real_questions.append(turn['question'])
      real_questions_repr.append(turn['repr'])

  real_questions_repr.append(main_q_repr)


  # iterate over synthetic questions and throw away the degenerate questions
  i = -1
  for tidx, turn in enumerate(q_list):
    high_similarity = False
    question = turn['question']
    answer = turn['answer']
    question_embedding = turn['repr']
    
    if turn['is_syn']:
      if question in real_questions:
        continue
        
      if len(answer.split()) == 1 and only_np:
        doc = nlp(str(answer.split()[0]))
        if doc[0].pos_ in ['ADJ', 'ADV']: # If the synthetic answer is adjective or adverb, pass, and only keep noun phrases
          continue			   # According to paper we only use noun phrase synthetic answers
        
      for q in real_questions_repr:
        if use_sim_threshold: # check for high similarity and repetitive questions, if use_sim_threshold is set to true
          if q @ question_embedding.T > gamma_threshold or len(question) < 4: # Further we throw away too short questions
            high_similarity = True
            break
      if high_similarity:
        continue
      
      # compute a score for each synthetic question
      augmented_history.append([question, 
                                tidx, 
                                question_embedding @ real_questions_repr[i] + \
                                question_embedding @ real_questions_repr[i + 1]])
    else:
      i += 1
      augmented_history.append([question, tidx, 20])
      if i == len(real_questions_repr) - 1:
        break

  X = sorted(sorted(augmented_history, key=lambda x: x[2])[-q_num - M:], key=lambda x: x[1])
  
  best_qs = []
  for tidx, x in enumerate(X):
    question = x[0]
    if question not in real_questions:
      best_qs.append({
          'question': question,
          'is_syn': True,
          'answer': ''})
    else:
      i += 1
      best_qs.append({
          'question': question,
          'is_syn': False,
          'answer': ''
      })

  final_augmented_history = select_questions_based_on_dist(best_qs, q_num, syn_num)
  return final_augmented_history




def select_questions_based_on_dist(extra_hists, q_num, max_num=4):
  seen_q = -1
  madeup_hist = list()
  syn_questions = list()
  for extra_hist in extra_hists:
    if not extra_hist['is_syn']:
      seen_q += 1
    if seen_q == q_num:
      break

    if not extra_hist['is_syn']:
      madeup_hist.append([extra_hist['question'], extra_hist['answer'], True])
    elif extra_hist['is_syn']:
      syn_questions.append([extra_hist['question'], extra_hist['answer'], False, len(madeup_hist), [seen_q + 1]])
      
          
  if len(syn_questions) >= max_num:
    if dist_type == 'linear': # use linear distribution
      p = np.array([item[-1][0] for item in syn_questions])
      p = p / p.sum()
      selected_syn_questions_ids = np.random.choice(list(range(len(syn_questions))), max_num, replace=False, p=p)
    elif dist_type == 'uniform': # use uniform distribution
      selected_syn_questions_ids = random.sample(list(range(len(syn_questions))), max_num)
  else:
    selected_syn_questions_ids = random.sample(list(range(len(syn_questions))), len(syn_questions))
    

  selected_syn_questions_ids = sorted(selected_syn_questions_ids)
  for i, index in enumerate(selected_syn_questions_ids):
    madeup_hist.insert(syn_questions[index][3] + i, syn_questions[index]) # integrating synthetic questions
    
  # Putting ? at the end of questions, if not already included
  for hist in madeup_hist:
    if not hist[0].endswith('?'):
      hist[0] += '?'
  return madeup_hist
  
  
  
# a function to make examples from dataset
def make_examples(data, data_type, num_sample, clean_samples=True):
  examples = []
  each_file_size = 1000
  example_file_index = 0
  data_dir = f'examples/{data_type}/'

  for dialog_num, sample in enumerate(tqdm(data[ :num_sample], leave=False, position=0)):
    dialog_history = []
    dialog = sample['paragraphs'][0]
    context = dialog['context']
    if dialog['qas'][0]['id'].split('#')[0] not in extra_hists_dict.keys() and data_type == 'train':
      continue
    for q_num, qas in enumerate(dialog['qas']):
      history = []
      question = qas['question']
      human_answer = qas['orig_answer']
      qid = qas['id']
      answers = []
      
      if not question.endswith('?'):
        question += '?'
        
      if not question.endswith('?'):
        print(question)


      for answer in qas['answers']:
        answer_ = {}
        answer_['text'] = answer['text']
        answer_['start'] = answer['answer_start']
        answer_['end'] = answer['answer_start'] + len(answer['text'])
        answers.append(answer_)
        

      is_answerable = False if qas['answers'][0]['text'] == 'CANNOTANSWER' else True

      if not q_num == 0:
        history = deepcopy(dialog_history)

      cqa_example = CQA_DATA(question=question,
                             context=context,
                             history=history,
                             answers=answers,
                             human_answer=human_answer,
                             qid=qid,
                             q_num=q_num,
                             answer_start=answers[0]['start'],
                             answer_end=answers[0]['end'],
                             is_answerable=is_answerable)

      # build this extra one
      if data_type == 'train':
        # if dialog turn is above than threshold (tau), we will augment the history
        if q_num >= tau:
          extra_hist = make_new_history_with_heuristic(extra_hists_dict[qid.split('#')[0]], q_num, S)
        else:
          extra_hist = history
        cqa_example_extra = CQA_DATA(question=question,
                              context=context,
                              history=extra_hist,
                              answers=answers,
                              human_answer=human_answer,
                              qid=qid,
                              q_num=q_num,
                              answer_start=answers[0]['start'],
                              answer_end=answers[0]['end'],
                              is_answerable=is_answerable)
        cqa_example_extra = CleanSample(cqa_example_extra).clean() if clean_samples else cqa_example_extra

      cqa_example = CleanSample(cqa_example).clean() if clean_samples else cqa_example
      if data_type == 'train':
        examples.append([cqa_example, cqa_example_extra])
      else:
        examples.append(cqa_example)
      dialog_history.append([cqa_example.question, cqa_example.answers[0]['text']])

      if data_type == 'train':
        cqa_example_extra_qs = [i[0] for i in cqa_example_extra.history]
        for hist in cqa_example.history:
          try:
            assert hist[0] in cqa_example_extra_qs
          except:
            print(cqa_example_extra_qs)
            print(hist[0])
            assert hist[0] in cqa_example_extra_qs

    if (dialog_num + 1) % each_file_size == 0:
      filename = f'{data_type}_examples_' + str(example_file_index) + '.bin'
      save_data(examples, os.path.join(data_dir, filename))
      example_file_index += 1
      examples = []

  if examples != []:
    filename = f'{data_type}_examples_' + str(example_file_index) + '.bin'
    save_data(examples, os.path.join(data_dir, filename))








# make features for eval
def make_features_eval(data_type, ph_token):
  data_dir = f'examples/{data_type}/'
  example_files = os.listdir(data_dir)
  example_files = sorted([os.path.join(data_dir, example_file) for example_file in example_files], key=lambda x: int(x.split('.')[0].split('_')[-1]))
  features_list = []
  features_dir = f'features/{data_type}/'
  max_history_to_consider = 11

  for file_index, filename in enumerate(example_files):
    examples = load_data(filename)
    for example_1 in tqdm(examples, leave=False, position=0):
      example_features_1 = []
      concatenated_question_1 = []

      dialog_id = example_1.qid.split('#')[0]
      q_num = int(example_1.qid.split('#')[1])
      if dialog_id not in hist_chunk_dict.keys():
        hist_chunk_dict[dialog_id] = [[] for _ in range(12)]

      # concat history
      for hist in example_1.history[-max_history_to_consider:]:
        concatenated_question_1.append(hist[0])
      # append current question to concatenated question
      concatenated_question_1.append(example_1.question)
      # make string out of concatenated question
      concatenated_question_1 = ' '.join(concatenated_question_1)


      # tokenize example_1
      text_tokens_1 = tokenizer(
          concatenated_question_1,
          example_1.cleaned_context,
          max_length=model.config.max_position_embeddings,
          padding='max_length',
          truncation='only_second',
          return_overflowing_tokens=True,
          return_offsets_mapping=True,
          stride=128)

      # find start and end of context
      for idx in range(len(text_tokens_1['input_ids'])):
        found_start_1 = False
        found_end_1 = False
        context_start_1 = 0
        cintext_end_1 = 511
        max_context_dict_1 = {}
        # context start
        for token_idx, token in enumerate(text_tokens_1['offset_mapping'][idx][1:]):
          if token[0] == 0 and token[1] == 0:
            context_start_1 = token_idx + start_offset_
            break
        # context end
        for token_idx, token in enumerate(text_tokens_1['offset_mapping'][idx][context_start_1: ]):
          if token[0] == 0 and token[1] == 0:
            context_end_1 = token_idx + context_start_1 - 1
            break
        # max context dict
        chunk_offset_mapping = text_tokens_1['offset_mapping'][idx]
        for context_idx, data in enumerate(chunk_offset_mapping[context_start_1: context_end_1 + 1]):
          max_context_dict_1[f'({data[0]},{data[1]})'] = min(context_idx, context_end_1 - context_idx) + (context_end_1 - context_start_1 + 1) * .01


        # find and mark current question answer
        last_token = None
        for token_idx, token in enumerate(chunk_offset_mapping[context_start_1: context_end_1 + 1]):
          if token[0] == example_1.cleaned_answer['start'] and not found_start_1:
            found_start_1 = True
            start_1 = token_idx + context_start_1
          elif last_token and last_token[0] < example_1.cleaned_answer['start'] and token[0] > example_1.cleaned_answer['start']:
            found_start_1 = True
            start_1 = (token_idx - 1) + context_start_1
          if token[1] == example_1.cleaned_answer['end'] and not found_end_1:
            found_end_1 = True
            end_1 = token_idx + context_start_1
          elif last_token and last_token[1] < example_1.cleaned_answer['end'] and token[1] > example_1.cleaned_answer['end'] and last_token:
            found_end_1 = True
            end_1 = token_idx + context_start_1
          last_token = token
        # add feature to features list
        if ((not found_start_1) or (not found_end_1)):
          start_1, end_1 = 0, 0
        if end_1 < start_1 and found_start_1 and found_end_1:
          assert False, 'start and end do not match'


        # plausibility check
        if found_start_1 or found_end_1:
          answer = example_1.cleaned_answer['text'].strip()
          generated_answer = example_1.cleaned_context[chunk_offset_mapping[start_1][0]: chunk_offset_mapping[end_1][1]]



        # append to example features
        if found_start_1 and found_end_1:
          hist_chunk_dict[dialog_id][idx].append(q_num)
          T = deepcopy(hist_chunk_dict[dialog_id])
        else:
          T = None

        example_features_1.append(CoTaH_Feature(example_1.qid,
                                          idx,
                                          text_tokens_1['input_ids'][idx],
                                          text_tokens_1['attention_mask'][idx],
                                          text_tokens_1['token_type_ids'][idx],
                                          text_tokens_1['offset_mapping'][idx],
                                          T,
                                          max_context_dict_1,
                                          start_1,
                                          end_1,
                                          example_1.is_answerable,
                                          example_1.context,
                                          example_1.cleaned_context,
                                          context_start_1,
                                          context_end_1,
                                          example_1.answer_start,
                                          example_1.answer_end,
                                          example_1.answer))


      # create max context mask
      handle_max_context(example_features_1, context_start_1)

      features_list.append([example_features_1])

    filename = f'{data_type}_features_' + str(file_index) + '.bin'
    save_data(features_list, os.path.join(features_dir, filename))
    features_list = []





def make_features_train_for_each_example(example, concatenated_question):

  example_features = list()
  # tokenize current feature
  text_tokens = tokenizer(
      concatenated_question,
      example.cleaned_context,
      max_length=model.config.max_position_embeddings,
      padding='max_length',
      truncation='only_second',
      return_overflowing_tokens=True,
      return_offsets_mapping=True,
      stride=128)

  # find start and end of context
  for idx in range(len(text_tokens['input_ids'])):
    found_start = False
    found_end = False
    context_start = 0
    cintext_end = 511
    max_context_dict = {}

    for token_idx, token in enumerate(text_tokens['offset_mapping'][idx][1:]):
      if token[0] == 0 and token[1] == 0:
        context_start = token_idx + start_offset_
        break

    for token_idx, token in enumerate(text_tokens['offset_mapping'][idx][context_start:]):
      if token[0] == 0 and token[1] == 0:
        context_end = token_idx + context_start - 1
        break

    chunk_offset_mapping = text_tokens['offset_mapping'][idx]
    for context_idx, data in enumerate(chunk_offset_mapping[context_start: context_end + 1]):
      max_context_dict[f'({data[0]},{data[1]})'] = min(context_idx, context_end - context_idx) + (context_end - context_start + 1) * .01

    # find and mark current question answer
    marker_ids = np.zeros(shape=(model.config.max_position_embeddings,), dtype=np.int64)
    last_token = None
    for token_idx, token in enumerate(chunk_offset_mapping[context_start: context_end + 1]):
      if token[0] == example.cleaned_answer['start'] and not found_start:
        found_start = True
        start = token_idx + context_start

      elif last_token and last_token[0] < example.cleaned_answer['start'] and token[0] > example.cleaned_answer['start']:
        found_start = True
        start = (token_idx - 1) + context_start

      if token[1] == example.cleaned_answer['end'] and not found_end:
        found_end = True
        end = token_idx + context_start

      elif last_token and last_token[1] < example.cleaned_answer['end'] and token[1] > example.cleaned_answer['end'] and last_token:
        found_end = True
        end = token_idx + context_start
      last_token = token


    # add feature to features list
    if found_start and found_end and end < start:
      assert False, 'start and end do not match'

    # since there is no prediction we throw the example away (only when training) Following previous research codess 
    # This is only for the training part. We don't throw away anything from the eval/test set
    if ((not found_start) or (not found_end)):
      continue



    # plausibility check
    if found_start or found_end:
      answer = example.cleaned_answer['text'].strip()
      generated_answer = example.cleaned_context[chunk_offset_mapping[start][0]: chunk_offset_mapping[end][1]]
      if answer.find(generated_answer) == -1:
        pass

    # mark history answers
    example_features.append(CoTaH_Feature(example.qid,
                                      idx,
                                      text_tokens['input_ids'][idx],
                                      text_tokens['attention_mask'][idx],
                                      text_tokens['token_type_ids'][idx],
                                      text_tokens['offset_mapping'][idx],
                                      None,
                                      max_context_dict,
                                      start,
                                      end,
                                      example.is_answerable,
                                      example.context,
                                      example.cleaned_context,
                                      context_start,
                                      context_end,
                                      example.answer_start,
                                      example.answer_end,
                                      example.answer))
  return example_features, True

def make_features_train(data_type, ph_token):
  data_type = 'train'
  ph_token = '[PH]'
  data_dir = f'examples/{data_type}/'
  example_files = os.listdir(data_dir)
  example_files = sorted([os.path.join(data_dir, example_file) for example_file in example_files], key=lambda x: int(x.split('.')[0].split('_')[-1]))
  features_list = []
  features_dir = f'features/{data_type}/'
  max_history_to_consider = 11

  for file_index, filename in enumerate(example_files):
    examples = load_data(filename)
    for example_1, example_2 in tqdm(examples, leave=False, position=0):
      concatenated_question_1 = []
      concatenated_question_2 = []

      # concat history
      for hist in example_1.history[-max_history_to_consider:]:
        concatenated_question_1.append(hist[0])
      for hist in example_2.history[-max_history_to_consider:]:
        concatenated_question_2.append(hist[0])

      # append current question to concatenated question
      concatenated_question_1.append(example_1.question)
      concatenated_question_2.append(example_2.question)

      # make string out of concatenated question
      concatenated_question_1 = ' '.join(concatenated_question_1)
      concatenated_question_2 = ' '.join(concatenated_question_2)

      cq_1_len = len(tokenizer(concatenated_question_1, add_special_tokens=False)['input_ids'])
      cq_2_len = len(tokenizer(concatenated_question_2, add_special_tokens=False)['input_ids'])
      
      
      # We add dummy [PH] token so that we have the same length of question for the same example 
      question_pad_len = abs(cq_1_len - cq_2_len)
      if cq_1_len >= cq_2_len:
        concatenated_question_2 = concatenated_question_2 + ' ' + ' '.join([ph_token] * question_pad_len)
      elif cq_1_len <= cq_2_len:
        concatenated_question_1 = concatenated_question_1 + ' ' + ' '.join([ph_token] * question_pad_len)

      cq_1_len = len(tokenizer(concatenated_question_1, add_special_tokens=False)['input_ids'])
      cq_2_len = len(tokenizer(concatenated_question_2, add_special_tokens=False)['input_ids'])

      assert cq_1_len == cq_2_len

      example_features_1, _ = make_features_train_for_each_example(example_1, concatenated_question_1)
      example_features_2, _ = make_features_train_for_each_example(example_2, concatenated_question_2)
      
      # check whether the length of features for a single example to be the same
      assert len(example_features_1) == len(example_features_2)

      # checking everything for an example to be equal
      for example_feature_1, example_feature_2 in zip(example_features_1, example_features_2):
        assert example_feature_1.start == example_feature_2.start
        assert example_feature_1.end == example_feature_2.end
        assert example_feature_1.context_start == example_feature_2.context_start
        assert example_feature_1.context_end == example_feature_2.context_end

        handle_max_context(example_features_1, example_feature_1.context_start)
        handle_max_context(example_features_2, example_feature_2.context_start)

        features_list.append([example_feature_1, example_feature_2])

    filename = f'{data_type}_features_' + str(file_index) + '.bin'
    save_data(features_list, os.path.join(features_dir, filename))
    features_list = []




# Here, we split the dev (eval) set of QuAC to a new dev and test sets 
random.seed(SEED)
train_data = deepcopy(train_data['data'])
random.shuffle(train_data)
eval_test = deepcopy(eval_data['data'])
P = 0
eval_data, test_data = [], []
for x in eval_test:
  P += len(x['paragraphs'][0]['qas'])
num_eval = P / 2
Y = 0
for idx, x in enumerate(eval_test):
  eval_data.append(x)
  Y += len(x['paragraphs'][0]['qas'])
  if Y >= num_eval:
    break
test_data = deepcopy(eval_test[idx + 1: ])

if not is_preprocessed:
  make_examples(train_data, 'train', 50)
  make_features_train('train', '[PH]')

  make_examples(eval_data, 'eval', 100)
  make_features_eval('eval', '[PH]')

  make_examples(test_data, 'test', 100)
  make_features_eval('test', '[PH]')

  def wrtie_file(data, filename):
    with open(filename, 'w') as f:
        json.dump(data, f)

  wrtie_file({
      'data': eval_data
  }, orig_eval_file)

  wrtie_file({
      'data': test_data
  }, orig_test_file)







# QA Model
class QA_Model(nn.Module):

  def __init__(self, transformer, device):
    super(QA_Model, self).__init__()
    self.transformer = transformer
    self.start_end_head = nn.Linear(self.transformer.config.hidden_size, 2)
    nn.init.normal_(self.start_end_head.weight, mean=.0, std=.02)
    self.device = device

  def forward(self, x):
    for key in x:
      x[key] = x[key].to(device)
    # transformer output
    transformer_output = self.transformer(**x)
    start_end_logits = self.start_end_head(transformer_output.last_hidden_state)
    start_logits, end_logits = start_end_logits.split(1, dim=-1)
    start_logits = start_logits.squeeze(-1)
    end_logits = end_logits.squeeze(-1)
    return start_logits, end_logits



# building saving settings
save_dir_prefix, checkpoint_dir, log_dir, meta_log_file, \
loss_log_file, mean_f1_file_eval, mean_f1_file_test, \
scores_eval, scores_test, checkpoint_available, current_checkpoint = build_save_settings(args)




# Train loop
epochs = 3
lr = 3e-5
beta_1 = .9
beta_2 = .999
eps = 1e-6
accumulation_steps = 1
accumulation_counter = 0
device = 'cuda:0' if torch.cuda.is_available() else 'cpu' # device to be trained on
qa_model = QA_Model(model, device).to(device) # initiate the model
loss_fn = nn.CrossEntropyLoss().to(device)
kl_loss_fn = nn.KLDivLoss(reduction='none')
sup_loss_collection = [] # a list to store cross-entropy loss
kl_loss_collection = [] # a list to store consistency loss
train_dataloader = Cotah_DataLoader(current_file=0, current_index=0, batch_size=batch_size, shuffle=True, data_type='train')
eval_dataloader = Cotah_DataLoader(current_file=0, current_index=0, batch_size=1, shuffle=False, data_type='eval')
test_dataloader = Cotah_DataLoader(current_file=0, current_index=0, batch_size=1, shuffle=False, data_type='test')
each_step_log = 100
start_step = 0
current_file = 0
current_index = 0
best_f1 = 0

info = ''
info += f'lr {lr}\n'
info += f'epochs {epochs}\n'
info += f'batch_size {batch_size}\n'
info += f'accumulation_steps {accumulation_steps}\n'
info += f'kl_ratio {kl_ratio}\n'
info += f'S {S}\n'
info += f'M {M}\n'
info += f'Gamma {gamma_threshold}\n'
info += f'-----------------------------------------\n'

if not checkpoint_available:
  with open(meta_log_file, 'a') as f:
    f.write(info) # Saving the settings
  with open(loss_log_file, 'a') as f:
    f.write('')


# setting the optimizer and scheduler
optimization_steps = int(epochs * len(train_dataloader) / accumulation_steps)
epoch_steps = int(len(train_dataloader) / accumulation_steps)
warmup_ratio = .1
warmup_steps = int(optimization_steps * warmup_ratio)
optimizer = AdamW(qa_model.parameters(), lr=lr, betas=(beta_1, beta_2), eps=eps)
scheduler = get_linear_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=optimization_steps)


# laod checkpoint if available
if checkpoint_available:
  print('loading checkpoint')
  start_epoch, optimizer_dict, scheduler_dict, qa_model_dict = load_checkpoint(current_checkpoint)
  qa_model.load_state_dict(qa_model_dict)
  optimizer.load_state_dict(optimizer_dict)
  scheduler.load_state_dict(scheduler_dict)

current_file_index_ = current_file
train_dataloader.reset_dataloader(current_file, current_index)
qa_model.train()
for epoch in range(epochs):
  train_step = 1
  sup_loss_value = 0
  kl_loss_value = 0
  log_step = start_step

  for data in train_dataloader:
    if train_dataloader.data_manager.current_file_index != current_file_index_:
      current_file_index_ = train_dataloader.data_manager.current_file_index

    x, y = dict(), dict()
    x['input_ids'] = data.pop('input_ids_0').to(device)
    y['input_ids'] = data.pop('input_ids_1').to(device)
    x['attention_mask'] = data.pop('attention_mask_0').to(device)
    y['attention_mask'] = data.pop('attention_mask_1').to(device)
    x['token_type_ids'] = data.pop('token_type_ids_0').to(device)
    y['token_type_ids'] = data.pop('token_type_ids_1').to(device)

    start_positions_0 = data.pop('start_positions_0').to(device)
    start_positions_1 = data.pop('start_positions_1').to(device)
    end_positions_0 = data.pop('end_positions_0').to(device)
    end_positions_1 = data.pop('end_positions_1').to(device)
    features_0 = data.pop('features_0')
    features_1 = data.pop('features_1')
    cluster_size = data.pop('cluster_size')

    start_logits_0, end_logits_0 = qa_model(x) # run QA network with original history (left figure in the paper)
    with torch.no_grad(): # the gradient cut
      start_logits_1, end_logits_1 = qa_model(y) # run QA network with augmented history (right figure in the paper)

    # compute consistency loss
    kl_loss_list = []
    for features_idx, (start_logit_0, start_logit_1, end_logit_0, end_logit_1) in \
    		enumerate(zip(start_logits_0, start_logits_1, end_logits_0, end_logits_1)):
    
      # check the correctness of settings
      assert features_0[features_idx].context_start == features_0[features_idx].context_start
      assert features_1[features_idx].context_end == features_1[features_idx].context_end
      
      # start token consistency loss
      start_kl_loss = compute_kl(start_logit_0[features_0[features_idx].context_start: features_0[features_idx].context_end + 1], \
                                 start_logit_1[features_1[features_idx].context_start: features_1[features_idx].context_end + 1], \
                                 kl_loss_fn)
      # end token consistency loss                             
      end_kl_loss = compute_kl(end_logit_0[features_0[features_idx].context_start:  features_0[features_idx].context_end + 1], \
                               end_logit_1[features_1[features_idx].context_start:  features_1[features_idx].context_end + 1], \
                               kl_loss_fn)
      # add the current losses                         
      kl_loss_list.append(start_kl_loss)
      kl_loss_list.append(end_kl_loss)
    
    kl_loss_list = torch.stack(kl_loss_list) # stack the losses
    kl_loss = kl_loss_list.mean() * kl_ratio # compute final consistency loss

    # compute cross-entropy loss
    sup_loss = (loss_fn(start_logits_0, start_positions_0) + loss_fn(end_logits_0, end_positions_0)) / 2
    loss = sup_loss + kl_loss # compute the total loss
    loss = loss / accumulation_steps
    sup_loss_value += sup_loss.item()
    kl_loss_value += kl_loss.item()
    loss.backward() # compute the gradients
    accumulation_counter += 1

    # logging the loss every each_step_log steps
    if train_step % each_step_log == 0:
      print_loss(sup_loss_collection, epoch, epochs, log_step + 1, epoch_steps)
      print_loss(kl_loss_collection, epoch, epochs, log_step + 1, epoch_steps)
      print('----------------------------------------------')
      save_loss(sup_loss_collection, epoch, epochs, log_step + 1, epoch_steps)
      save_loss(kl_loss_collection, epoch, epochs, log_step + 1, epoch_steps)
      sup_loss_collection = []
      kl_loss_collection = []

    
    # run the optimizer & scheduler
    if accumulation_counter % accumulation_steps == 0:
      sup_loss_collection.append(sup_loss_value)
      kl_loss_collection.append(kl_loss_value)
      sup_loss_value = 0
      kl_loss_value = 0
      log_step += 1
      optimizer.step()
      scheduler.step()
      optimizer.zero_grad()
      torch.cuda.empty_cache()
      accumulation_counter = 0

    train_step += 1


  # save the checkpoint of the QA network and run it on the dev (eval) set
  save_checkpoint(epoch + 1, checkpoint_dir, qa_model, optimizer, scheduler)
  qa_model.eval()
  print('-------------------- Evaluation --------------------')
  eval_p = EvalProcessOutput()
  with torch.no_grad():
    for step, data in enumerate(eval_dataloader):
      start_positions = data.pop('start_positions')
      end_positions = data.pop('end_positions')
      features = data.pop('features')
      cluster_size = data.pop('cluster_size')
      start_logits, end_logits = qa_model(data)
      eval_p.process_feature_output(to_numpy(start_logits),
                                    to_numpy(end_logits),
                                    features)
  eval_p.process_output()
  filename = os.path.join(log_dir, f'val_{epoch + 1}_preds.json')
  write_to_file(eval_p.dialogs_answers, filename)
  res__ = run_eval(filename, orig_eval_file)
  if res__['f1'] > best_f1:
    best_f1 = res__['f1']
    best_checkpoint = f'checkpoint_{epoch + 1}'
  save_results(scores_eval, res__)
  print_mean(res__['turn_f1s'], mean_f1_file_eval)
  qa_model.train()



# run the QA network test set with the best checkpoint
qa_model.load_state_dict(torch.load(os.path.join(checkpoint_dir, best_checkpoint))['model_dict'])
qa_model.eval()
print('-------------------- Test --------------------')
test_p = EvalProcessOutput()
with torch.no_grad():
  for step, data in enumerate(test_dataloader):
    start_positions = data.pop('start_positions')
    end_positions = data.pop('end_positions')
    features = data.pop('features')
    cluster_size = data.pop('cluster_size')
    start_logits, end_logits = qa_model(data)
    test_p.process_feature_output(to_numpy(start_logits),
                                  to_numpy(end_logits),
                                  features)
test_p.process_output()
filename = os.path.join(log_dir, f'test_preds.json')
write_to_file(test_p.dialogs_answers, filename)
res__ = run_eval(filename, orig_test_file)
save_results(scores_test, res__)
print_mean(res__['turn_f1s'], mean_f1_file_test)
